{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# $$\\text{FRONT-END Execution Code}$$\n",
    "\n",
    "Note: Change path to mathBot directory\n",
    "\n",
    "(THIS FILE WILL BE EXECUTED BY THE FRONT-END TO RUN THE BACK-END CODE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "# import spacy\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "else:\n",
    "    print(\"No GPUs available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('exam_final_pickle.pkl', 'rb') as f:\n",
    "  df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(df['Question'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacify(s):\n",
    "    return ' '.join(list(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = list(df['Equation'].apply(lambda y: spacify(y)).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_X(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([?.!,â€™])\", r\" \\1 \", s)\n",
    "    s = re.sub(r\"([0-9])\", r\" \\1 \", s)\n",
    "    s = re.sub(r'[\" \"]+', \" \", s)\n",
    "    s = s.rstrip().strip()\n",
    "    return s\n",
    "\n",
    "def preprocess_Y(e):\n",
    "    e = e.lower().strip()\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pp = list(map(preprocess_X, X))\n",
    "Y_pp = list(map(preprocess_Y, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert into tensorflow-gpu format\n",
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor, X_lang_tokenizer = tokenize(X_pp)\n",
    "Y_tensor, Y_lang_tokenizer = tokenize(Y_pp)\n",
    "previous_length = len(Y_lang_tokenizer.word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_head_tail(x, last_int):\n",
    "    l = []\n",
    "    l.append(last_int + 1)\n",
    "    l.extend(x)\n",
    "    l.append(last_int + 2)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor_list = [append_head_tail(i, len(X_lang_tokenizer.word_index)) for i in X_tensor]\n",
    "Y_tensor_list = [append_head_tail(i, len(Y_lang_tokenizer.word_index)) for i in Y_tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = tf.keras.preprocessing.sequence.pad_sequences(X_tensor_list, padding='post')\n",
    "Y_tensor = tf.keras.preprocessing.sequence.pad_sequences(Y_tensor_list, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21',\n",
    "        '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33',\n",
    "        '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45',\n",
    "        '46', '47', '48', '49', '50']\n",
    "\n",
    "for idx,key in enumerate(keys):\n",
    "    Y_lang_tokenizer.word_index[key] = len(Y_lang_tokenizer.word_index) + idx + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor_train, X_tensor_test, Y_tensor_train, Y_tensor_test = train_test_split(X_tensor, Y_tensor, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SET_SIZE = len(X_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = np.floor(TRAINING_SET_SIZE/BATCH_SIZE)\n",
    "\n",
    "data = tf.data.Dataset.from_tensor_slices((X_tensor_train, Y_tensor_train)).shuffle(TRAINING_SET_SIZE)\n",
    "data = data.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "num_layers = 4\n",
    "d_model = 128       # Embedding dimension\n",
    "dff = 512           # Dimensionality of inner-layer of FNN\n",
    "num_heads = 8       # Number of parallel attention layers (heads)\n",
    "dropout_rate = 0.1\n",
    "\n",
    "X_vocabulary_size = len(X_lang_tokenizer.word_index) + 3\n",
    "Y_vocabulary_size = len(Y_lang_tokenizer.word_index) + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Trial VALUES -> WONT WORK FOR NOW\n",
    "# TRAINING_SET_SIZE = len(X_tensor_train)\n",
    "# BATCH_SIZE = 128\n",
    "# steps_per_epoch = np.floor(TRAINING_SET_SIZE/BATCH_SIZE)\n",
    "\n",
    "# data = tf.data.Dataset.from_tensor_slices(\n",
    "#     (X_tensor_train, Y_tensor_train)).shuffle(TRAINING_SET_SIZE)\n",
    "# data = data.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# num_layers = 6\n",
    "# d_model = 256       # Embedding dimension\n",
    "# dff = 1024          # Dimensionality of inner-layer of FNN\n",
    "# num_heads = 16      # Number of parallel attention layers (heads)\n",
    "# dropout_rate = 0.2\n",
    "\n",
    "# X_vocabulary_size = len(X_lang_tokenizer.word_index) + 3\n",
    "# Y_vocabulary_size = len(Y_lang_tokenizer.word_index) + 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "X_batch_example, Y_batch_example = next(iter(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "    \n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    \n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "        \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    \n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "    \n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "            \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "        \n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "        \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                    (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "            \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        # normalize data per feature instead of batch\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        # Multi-head attention layer\n",
    "        attn_output, _ = self.mha(x, x, x, mask) \n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        # add residual connection to avoid vanishing gradient problem\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        \n",
    "        # Feedforward layer\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        # add residual connection to avoid vanishing gradient problem\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "        \n",
    "        # Create encoder layers (count: num_layers)\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                        for _ in range(num_layers)]\n",
    "    \n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "            \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  \n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "    \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "        \n",
    "    def call(self, x, enc_output, training, \n",
    "            look_ahead_mask, padding_mask):\n",
    "\n",
    "        # Masked multihead attention layer (padding + look-ahead)\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        # again add residual connection\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        \n",
    "        # Masked multihead attention layer (only padding)\n",
    "        # with input from encoder as Key and Value, and input from previous layer as Query\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        # again add residual connection\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "        \n",
    "        # Feedforward layer\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        # again add residual connection\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "                maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "        \n",
    "        # Create decoder layers (count: num_layers)\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                        for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, enc_output, training, \n",
    "            look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        \n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        \n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        \n",
    "        x += self.pos_encoding[:,:seq_len,:]\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                look_ahead_mask, padding_mask)\n",
    "        \n",
    "        # store attenion weights, they can be used to visualize while translating\n",
    "        attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "        attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "        \n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "                target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                            input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                            target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "        \n",
    "    def call(self, inp, tar, training, enc_padding_mask, \n",
    "            look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "        # Pass the input to the encoder\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
    "        \n",
    "        # Pass the encoder output to the decoder\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "        \n",
    "        # Pass the decoder output to the last linear layer\n",
    "        final_output = self.final_layer(dec_output)\n",
    "        \n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "# Adam optimizer with a custom learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    # Apply a mask to paddings (0)\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    accuracies = tf.equal(tf.cast(real, dtype=tf.int32), tf.cast(tf.argmax(pred, axis=2), dtype=tf.int32))\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(tf.cast(real, dtype=tf.int32), 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "train_accuracy_mean = tf.keras.metrics.Mean(name='train_accuracy_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          X_vocabulary_size, Y_vocabulary_size, \n",
    "                          pe_input=X_vocabulary_size, \n",
    "                          pe_target=Y_vocabulary_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    # Encoder padding mask (Used in the 2nd attention block in the decoder too.)\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by\n",
    "    # the decoder.\n",
    "    # Look ahead mask (for hiding the rest of the sequence in the 1st decoder attention layer)\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, look_ahead_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Directory path here\n",
    "drive_root = \"C:\\\\-----\\\\MathBot\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_directory = os.path.join(drive_root, \"MLProject\\\\checkpoints\")\n",
    "checkpoint_directory = os.path.join(checkpoint_directory, \"training_checkpoints\\\\moops_transfomer\")\n",
    "\n",
    "print(\"Checkpoints directory is\", checkpoint_directory)\n",
    "if os.path.exists(checkpoint_directory):\n",
    "    print(\"Checkpoints folder already exists!\")\n",
    "else:\n",
    "    print(\"Creating a checkpoints directory...\")\n",
    "    os.makedirs(checkpoint_directory)\n",
    "\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(checkpoint, checkpoint_directory, max_to_keep=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_checkpoint = ckpt_manager.latest_checkpoint\n",
    "last_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if last_checkpoint:\n",
    "    current_epoch_num = int(last_checkpoint.split('/')[-1].split('-')[-1])\n",
    "    checkpoint.restore(last_checkpoint)\n",
    "    print('Last checkpoint has been restored!')\n",
    "else:\n",
    "    current_epoch_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(\n",
    "        inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp,\n",
    "                                     True,\n",
    "                                     enc_padding_mask,\n",
    "                                     look_ahead_mask,\n",
    "                                     dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)\n",
    "    train_accuracy_mean(accuracy_function(tar_real, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(current_epoch_num, EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    train_accuracy_mean.reset_states()\n",
    "    \n",
    "    # inp -> math problem, tar -> expression\n",
    "    for (batch, (inp, tar)) in enumerate(data):\n",
    "        train_step(inp, tar)\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print (f'Epoch {epoch + 1}, Batch {batch}, Loss {train_loss.result():.5f},\\\n",
    "             SC Accuracy {train_accuracy.result():.5f}, Mean Accuracy {train_accuracy_mean.result():.5f}')\n",
    "        \n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print (f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
    "        \n",
    "    print (f'Epoch {epoch + 1}, Loss {train_loss.result():.5f},\\\n",
    "     SC Accuracy {train_accuracy.result():.5f}, Mean Accuracy {train_accuracy_mean.result():.5f}')\n",
    "\n",
    "    print (f'Training time for this epoch: {(time.time() - start):.5f} seconds\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_problem):\n",
    "    start_token = [len(X_lang_tokenizer.word_index)+1]\n",
    "    end_token = [len(X_lang_tokenizer.word_index)+2]\n",
    "    \n",
    "    # input_problem is the word problem, hence adding the start and end token\n",
    "    input_problem = start_token + [X_lang_tokenizer.word_index[i] for i in preprocess_X(input_problem).split(' ')] + end_token\n",
    "    encoder_input = tf.expand_dims(input_problem, 0)\n",
    "    \n",
    "    # start with expression's start token\n",
    "    decoder_input = [previous_length+1]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "        \n",
    "    for i in range(MAX_LENGTH):\n",
    "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
    "    \n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                    output,\n",
    "                                                    False,\n",
    "                                                    enc_padding_mask,\n",
    "                                                    look_ahead_mask,\n",
    "                                                    dec_padding_mask)\n",
    "        \n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :] \n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), dtype=tf.int32)\n",
    "        \n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == previous_length + 2:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "        \n",
    "        # concatenate the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_weights(attention, problem, result, layer):\n",
    "    fig = plt.figure(figsize=(8, 16))\n",
    "    \n",
    "    sentence = preprocess_X(problem)\n",
    "    \n",
    "    attention = tf.squeeze(attention[layer], axis=0)\n",
    "    \n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(4, 2, head+1)\n",
    "        \n",
    "        # plot the attention weights\n",
    "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "        \n",
    "        fontdict = {'fontsize': 11}\n",
    "        \n",
    "        ax.set_xticks(range(len(sentence.split(' '))+2))\n",
    "        ax.set_yticks(range(len([Y_lang_tokenizer.index_word[i] for i in list(result.numpy()) \n",
    "                            if i < len(Y_lang_tokenizer.word_index) and i not in [0,previous_length+1,previous_length+2]])+3))\n",
    "        \n",
    "        \n",
    "        ax.set_ylim(len([Y_lang_tokenizer.index_word[i] for i in list(result.numpy()) \n",
    "                            if i < len(Y_lang_tokenizer.word_index) and i not in [0,previous_length+1,previous_length+2]]), -0.5)\n",
    "            \n",
    "        ax.set_xticklabels(\n",
    "            ['<start>']+sentence.split(' ')+['<end>'], \n",
    "            fontdict=fontdict, rotation=90)\n",
    "        \n",
    "        ax.set_yticklabels([Y_lang_tokenizer.index_word[i] for i in list(result.numpy()) \n",
    "                            if i < len(Y_lang_tokenizer.word_index) and i not in [0,previous_length+1,previous_length+2]], \n",
    "                        fontdict=fontdict)\n",
    "        \n",
    "        ax.set_xlabel(f'Head {head+1}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(problem, plot='', plot_Attention_Weights=False):\n",
    "    Gtrans = Translator()\n",
    "    problem = Gtrans.translate(problem, dest='en').text\n",
    "    print(\"Translated Sentence: \", problem)\n",
    "    prediction, attention_weights = evaluate(problem)\n",
    "    predicted_expression = [Y_lang_tokenizer.index_word[i]\n",
    "                            for i in list(prediction.numpy())\n",
    "                            if (i < len(Y_lang_tokenizer.word_index) and i not in [0, 44, 45, 46, 47])]\n",
    "    print(f'Input: {problem}')\n",
    "    print('Predicted translation: {}'.format(' '.join(predicted_expression)))\n",
    "\n",
    "    if plot_Attention_Weights:\n",
    "        plot_attention_weights(attention_weights, problem, prediction, plot)\n",
    "\n",
    "    return predicted_expression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_testset(input_problem):\n",
    "    start_token = [len(X_lang_tokenizer.word_index)+1]\n",
    "    end_token = [len(X_lang_tokenizer.word_index)+2]\n",
    "    \n",
    "    # input_problem is the word problem, hence adding the start and end token\n",
    "    input_problem = start_token + list(input_problem.numpy()[0]) + end_token\n",
    "    encoder_input = tf.expand_dims(input_problem, 0)\n",
    "    \n",
    "    # start with expression's start token\n",
    "    decoder_input = [previous_length+1]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "        \n",
    "    for i in range(MAX_LENGTH):\n",
    "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
    "    \n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                    output,\n",
    "                                                    False,\n",
    "                                                    enc_padding_mask,\n",
    "                                                    look_ahead_mask,\n",
    "                                                    dec_padding_mask)\n",
    "        \n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :] \n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), dtype=tf.int32)\n",
    "        \n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == previous_length + 2:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "        \n",
    "        # concatenate the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = tf.data.Dataset.from_tensor_slices((X_tensor_test, Y_tensor_test)).shuffle(len(X_tensor_test))\n",
    "data_test = data_test.batch(1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Solve(input):\n",
    "    ans = solve(input)\n",
    "    exp = ','.join(ans).replace(',', '')\n",
    "    try:\n",
    "        val = exp.split('=')[1].strip()\n",
    "        expression = eval(val)\n",
    "    except:\n",
    "        val = exp.split('=')[0].strip()\n",
    "        expression = eval(val)\n",
    "    return val+\" = \"+str(expression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranSentence(sentence):\n",
    "    translator = Translator()\n",
    "    language = translator.detect(sentence)\n",
    "    if language.lang != \"en\":\n",
    "        return translator.translate(sentence, dest=\"en\").text\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Solve(input):\n",
    "#     try:\n",
    "#         ans=solve(input)\n",
    "#         exp = ','.join(ans).replace(',', '').split('=')[1]\n",
    "#         expression = eval(exp)\n",
    "#         return exp+\" = \"+str(expression)\n",
    "#     except KeyError:\n",
    "#         try:\n",
    "#             pass\n",
    "#             #chatGPT thing\n",
    "#         except:\n",
    "#             return \"Sorry, Please Rephrase the Question\"\n",
    "#     except:\n",
    "#         return \"Sorry, Please Rephrase the Question\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MathbotGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
